{
  "manifests": [
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fdb525acaa8d3db0f87c5577901f53ee",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt",
        "weights_checksum": "29eb495b11613825c1900382f5286963",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel"
      },
      "name": "BVLC-AlexNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
        "https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model was trained by Sergio Guadarrama @sguada\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4a7281748d91b960ba86d681010d4aa2",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_googlenet/deploy.prototxt",
        "weights_checksum": "dc05671f0443708da1b0d0ae0e0c9cad",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel"
      },
      "name": "BVLC-GoogLeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is the result of following the Caffe ImageNet model training instructions. It is a replication of the model described in the AlexNet publication with some differences. Differences: not training with the relighting data-augmentation; the order of pooling and normalization layers is switched (in CaffeNet, pooling is done before normalization). This model is snapshot of iteration 310,000. The best validation performance during training was iteration 313,000 with validation accuracy 57.412% and loss 1.82328. This model obtains a top-1 accuracy 57.4% and a top-5 accuracy 80.4% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy still.) This model was trained by Jeff Donahue @jeffdonahue\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "955051d11e44bd29dd87a25dd766ec23",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_reference_caffenet/deploy.prototxt",
        "weights_checksum": "af678f0bd3cdd2437e35679d88665170",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel"
      },
      "name": "BVLC-Reference-CaffeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pure Caffe instantiation of the R-CNN model for ILSVRC13 detection. This model was made by transplanting the R-CNN SVM classifiers into a fc-rcnn classification layer, provided here as an off-the-shelf Caffe detector. Try the detection example to see it in action. N.B. For research purposes, make use of the official R-CNN package and not this example. This model was trained by Ross Girshick @rbgirshick\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "487294811977b91daa2a5e97651bb85c",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_reference_rcnn_ilsvrc13/deploy.prototxt",
        "weights_checksum": "42c1556d2d47a9128c4a90e0a9c5341c",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_reference_rcnn_ilsvrc13.caffemodel"
      },
      "name": "BVLC-Reference-RCNN-ILSVRC13",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_rcnn_ilsvrc13",
        "https://arxiv.org/abs/1311.2524",
        "http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "db2d53bd6c310c1e2f83ae34341168a2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn68/deploy_dpn68-extra_no_ceil_mode.prototxt",
        "weights_checksum": "d8992cc89428578d65f930cdc6f52c81",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn68/dpn68-extra.caffemodel"
      },
      "name": "DPN68",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2c6116168692c8e37d6359141d39efa7",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn92/deploy_dpn92_no_ceil_mode.prototxt",
        "weights_checksum": "8ad72a531836f7f7c27e4f6d4c555b85",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn92/dpn92.caffemodel"
      },
      "name": "DPN92",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "two stream, 16 pixel prediction stride net, scoring 65.0 mIU on seg11valid trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models. These models are trained using extra data from Hariharan et al., but excluding SBD val. FCN-31s is fine-tuned from the ILSVRC-trained VGG-16 model, and the finer strides are then fine-tuned in turn. The \"at-once\" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "UNKNOWN",
      "model": {
        "graph_path": "https://raw.githubusercontent.com/shelhamer/fcn.berkeleyvision.org/master/voc-fcn16s/val.prototxt",
        "weights_path": "http://dl.caffe.berkeleyvision.org/fcn16s-heavy-pascal.caffemodel"
      },
      "name": "FCN-16s PASCAL",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1605.06211",
        "http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html",
        "https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn32s"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "single stream, 32 pixel prediction stride net, scoring 63.6 mIU on seg11valid. trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models. These models are trained using extra data from Hariharan et al., but excluding SBD val. FCN-32s is fine-tuned from the ILSVRC-trained VGG-16 model, and the finer strides are then fine-tuned in turn. The \"at-once\" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "UNKNOWN",
      "model": {
        "graph_path": "https://raw.githubusercontent.com/shelhamer/fcn.berkeleyvision.org/master/voc-fcn32s/val.prototxt",
        "weights_path": "http://dl.caffe.berkeleyvision.org/fcn32s-heavy-pascal.caffemodel"
      },
      "name": "FCN-32s PASCAL",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1605.06211",
        "http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html",
        "https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn32s"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f6232b561b2ffb1ff2d7c4ff6957f66b",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v3/deploy_inception-v3.prototxt",
        "weights_checksum": "0516c5ad05b50f6c71a8891a0cb6c4e8",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v3/inception-v3.caffemodel"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v4 which has a more uniform  simplified  architecture  and  more  inception  modules than Inception-v3.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7d285a61595715cd7d0a503c7df1be02",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v4/deploy_inception-v4.prototxt",
        "weights_checksum": "c7bd2e74e0ab376671af6c388f622e80",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v4/inception-v4.caffemodel"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/kentsommer/keras-inceptionV4/issues/5"
      ],
      "version": "4.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f4bfcf57b19470532cc3b244aec735e3",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-resnet-v2/deploy_inception-resnet-v2.prototxt",
        "weights_checksum": "6cda7beca27bb0c9e981ce21d5f27b18",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-resnet-v2/inception-resnet-v2.caffemodel"
      },
      "name": "Inception-ResNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "39865f78d0f94aad33c6bc8a779ff343",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inceptionbn-21k/deploy.prototxt",
        "weights_checksum": "6bf65186cee08b47ba6a767f5a5c8ce1",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inceptionbn-21k/Inception21k.caffemodel"
      },
      "name": "InceptionBN-21K",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/pertusa/InceptionBN-21K-for-Caffe",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 58.8% Top-1 Accuracy and 81.3% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "non-commercial",
      "model": {
        "graph_checksum": "8c749056e1c3481e118d719a3344e590",
        "graph_path": "deploy.prototxt",
        "weights_checksum": "b568958c0dcf1d97cbcff4c22b02a2be",
        "weights_path": "nin_imagenet.caffemodel"
      },
      "name": "Network in Network",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/mavenlin/d802a5849de39225bcc6",
        "https://arxiv.org/abs/1312.4400"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3ec3d1a5d432ef54cb2d1afc9f83403a",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101/ResNet-101-deploy.prototxt",
        "weights_checksum": "3f8ccc93329ddc280b91efae09f71973",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101/ResNet-101-model.caffemodel"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "90e3a68da9f3eca03270487abc1ec4dd",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101-v2/deploy_resnet101-v2.prototxt",
        "weights_checksum": "780febbf5c47e40546d0d75a75cc8659",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101-v2/resnet101-v2.caffemodel"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3d2b961982298ab8400319cc83d34e73",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152/ResNet-152-deploy.prototxt",
        "weights_checksum": "654892a23df300ca47ebfe66b4cfaa1b",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152/ResNet-152-model.caffemodel"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e2eb074fe56c4854ad10938e37d76500",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152-v2/deploy_resnet152-v2.prototxt",
        "weights_checksum": "982360ab64c0a1a5ed7efafd84c55d0a",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152-v2/resnet152-v2.caffemodel"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b06f1f1dd4155c71108d819155956a98",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet18-priv/deploy_resnet18-priv_no_ceil_mode.prototxt",
        "weights_checksum": "0904d601fc930d4f0c62a2a95b3c3b93",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet18-priv/resnet18-priv.caffemodel"
      },
      "name": "ResNet18",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "05f07aed80e87d63db734f3f28a2b1f1",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet269-v2/deploy_resnet269-v2.prototxt",
        "weights_checksum": "d984f4774cf865728219ada8bfec0094",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet269-v2/resnet269-v2.caffemodel"
      },
      "name": "ResNet269",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4ba3f945e7b86b07648e4f4351de0699",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet50/ResNet-50-deploy.prototxt",
        "weights_checksum": "44b20660c5948391734036963e855dd2",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet50/ResNet-50-model.caffemodel"
      },
      "name": "ResNet50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ec0c8e578040cbf76056deb41ba0af7d",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext101-32x4d/deploy_resnext101-32x4d_no_ceil_mode.prototxt",
        "weights_checksum": "5e5ec49b00d9f16d14ac6555dc992fcf",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext101-32x4d/resnext101-32x4d.caffemodel"
      },
      "name": "ResNeXt101-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "dde4af4f993b0db6c9b6dfec8fc3e615",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext26-32x4d-priv/deploy_resnext26-32x4d-priv_no_ceil_mode.prototxt",
        "weights_checksum": "a1d0f76137c4194aa4198df3a67fff26",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext26-32x4d-priv/resnext26-32x4d-priv.caffemodel"
      },
      "name": "ResNeXt26-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "bc71435f041d7f9eb5422e8d964b1dd3",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext50-32x4d/deploy_resnext50-32x4d_no_ceil_mode.prototxt",
        "weights_checksum": "d7ba773156d69ddb189aaf643356b9c5",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext50-32x4d/resnext50-32x4d.caffemodel"
      },
      "name": "ResNeXt50-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Squeezenet achieves similar results to AlexNet, at 50x fewer parameters and 1/500th the size. Small models are more feasible to deploy on hardware with limited memory, require less communication during distributed training, and are easier to distribute to clients. SqueezeNet achieves 57.5% and 80.3% top-1 and top-5 acuracty on ImageNet.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "ec33edbf17e082b3f5735fecc5a3d974",
        "graph_path": "deploy.prototxt",
        "weights_checksum": "bb9a2fd4be158e5b1e58a5cdc2b4aaa8",
        "weights_path": "squeezenet_v1.0.caffemodel"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "SqueezeNet v1.1 has 2.4x less computation than v1.0, without sacrificing accuracy.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "1e67cb171e16a4d66fd331409d2f7b0d",
        "graph_path": "https://raw.githubusercontent.com/rai-project/carml-models/master/caffe/squeezenet_v1.1/deploy.prototxt",
        "weights_checksum": "0357e4e11d173c72a01615888826bc8e",
        "weights_path": "https://github.com/DeepScale/SqueezeNet/raw/master/SqueezeNet_v1.1/squeezenet_v1.1.caffemodel"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360",
        "https://github.com/dividiti/ck-caffe/tree/master/package/caffemodel-deepscale-squeezenet-1.1",
        "http://cknowledge.org/repo/web.php?template=cknowledge\u0026\u0026wcid=package:59414348b1bdedb5"
      ],
      "version": "1.1"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 16-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "c70550f8203a4eaae53d7c39ef34c92d",
        "graph_path": "https://gist.githubusercontent.com/ksimonyan/211839e770f7b538e2d8/raw/ded9363bd93ec0c770134f4e387d8aaaaa2407ce/VGG_ILSVRC_16_layers_deploy.prototxt",
        "weights_checksum": "441315b0ff6932dbfde97731be7ca852",
        "weights_path": "http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/211839e770f7b538e2d8",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The MSO dataset"
      },
      "description": "The following model are finetuned on the Salient Object Subitizing dataset (~5000 images) with bounding box annotations. CNN models for the following CVPR'16 paper- Unconstrained Salient Object Detection via Proposal Subset Optimization J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price and R. Mech. CVPR, 2016.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "eac6d2978cff65319c7ef34aa89d760c",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sod/deploy.prototxt",
        "weights_checksum": "9a7dce6fc980c29d962f7c2758b2761d",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sod/VGG16_SOD_finetune.caffemodel"
      },
      "name": "VGG16_SOD",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/509111f8a00a9ece2c3d5dde6a750129",
        "https://github.com/jimmie33/SOD",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "http://cs-people.bu.edu/jmzhang/sod.html",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The SOS Dataset"
      },
      "description": "VGG16 finetuned on the Salient Object Subitizing (SOS) dataset, which is described in the CVPR'15 paper: \"Salient Object Subitizing\"\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY-NC 4.0",
      "model": {
        "graph_checksum": "eb820e9e4d6f35a961666acf0f2f15a2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sos/deploy.prototxt",
        "weights_checksum": "fa7886a2fcf21b0c49fe2f2b20fe5643",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sos/VGG16_SalObjSub.caffemodel"
      },
      "name": "VGG16_SOS",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/27c1c0a7736ba66c2395",
        "http://cs-people.bu.edu/jmzhang/sos.html",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 19-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "ccbbdda59210208be39f8974f5b5765e",
        "graph_path": "https://gist.githubusercontent.com/ksimonyan/3785162f95cd2d5fee77/raw/f43eeefc869d646b449aa6ce66f87bf987a1c9b5/VGG_ILSVRC_19_layers_deploy.prototxt",
        "weights_checksum": "b5c644beabd7cf06bdd9065cfd674c97",
        "weights_path": "http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/3785162f95cd2d5fee77",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model was used for experiments with Wide Residual Networks (BMVC 2016) http://arxiv.org/abs/1605.07146 by Sergey Zagoruyko and Nikos Komodakis. Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts\n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "efd258e3c28cab1f0e0a36c77aab2dec",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/wrn50-2/deploy_wrn50-2_no_ceil_mode.prototxt",
        "weights_checksum": "ea6c90d2f16a140d038359626bf78211",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/wrn50-2/wrn50-2.caffemodel"
      },
      "name": "WRN50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1605.07146",
        "https://github.com/szagoruyko/wide-residual-networks",
        "https://github.com/szagoruyko/functional-zoo/blob/master/imagenet-validation.py#L69-L88"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. \n",
      "framework": {
        "name": "Caffe",
        "version": "1.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d0df3aaaa43e4b82e4794b89420b7067",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/xception/deploy_xception.prototxt",
        "weights_checksum": "cad1c76e17becaeb0bde0351cfb6dba8",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/xception/xception.caffemodel"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/BVLC/caffe/pull/5665/files",
        "https://github.com/fchollet/deep-learning-models/blob/master/xception.py",
        "https://arxiv.org/abs/1610.02357"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7e9acac07a5cc7b8af010805ca8f7005",
        "graph_path": "predict_net.pb",
        "weights_checksum": "fc56c459e230573657037a0ad50d165f",
        "weights_path": "init_net.pb"
      },
      "name": "BVLC-AlexNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model is converted from the Caffe model.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "8b18c0c6d4cc85eca157b0e8d81e2f94",
        "graph_path": "predict_net.pb",
        "weights_checksum": "5b002c7f5528029d77d650c872696ea4",
        "weights_path": "init_net.pb"
      },
      "name": "BVLC-GoogLeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/pdf/1409.4842.pdf",
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is the result of following the Caffe ImageNet model training instructions. It is a replication of the model described in the AlexNet publication with some differences. Differences: not training with the relighting data-augmentation; the order of pooling and normalization layers is switched (in CaffeNet, pooling is done before normalization). This model is snapshot of iteration 310,000. The best validation performance during training was iteration 313,000 with validation accuracy 57.412% and loss 1.82328. This model obtains a top-1 accuracy 57.4% and a top-5 accuracy 80.4% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy still.) This model was trained by Jeff Donahue @jeffdonahue\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "165ea54221cc40ec7f14e518178d6845",
        "graph_path": "predict_net.pb",
        "weights_checksum": "1c2b24cafc6e6013152184c765ce8fe9",
        "weights_path": "init_net.pb"
      },
      "name": "BVLC-Reference-CaffeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pure Caffe instantiation of the R-CNN model for ILSVRC13 detection. This model was made by transplanting the R-CNN SVM classifiers into a fc-rcnn classification layer, provided here as an off-the-shelf Caffe detector. Try the detection example to see it in action. N.B. For research purposes, make use of the official R-CNN package and not this example. This model was trained by Ross Girshick @rbgirshick\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "960b6ea7a44230d75ff95a737904bd43",
        "graph_path": "predict_net.pb",
        "weights_checksum": "b58cdeed9d117ebf31badc87e7edf71e",
        "weights_path": "init_net.pb"
      },
      "name": "BVLC-Reference-RCNN-ILSVRC13",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_rcnn_ilsvrc13",
        "https://arxiv.org/abs/1311.2524",
        "http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c9bf7691d7b62476050b6c70c0da27a2",
        "graph_path": "predict_net.pb",
        "weights_checksum": "50a675be44bf0e809e0e0ebbeb101ab7",
        "weights_path": "init_net.pb"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "More uniform  simplified  architecture  and  more  inception  modules than Inception-v3. Achieved 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4ce199ddd7d1073195bdb48f09585513",
        "graph_path": "predict_net.pb",
        "weights_checksum": "b9947a80abc059b1564fd914ac6b576d",
        "weights_path": "init_net.pb"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/kentsommer/keras-inceptionV4/issues/5",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "4.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "this model is a pretrained model on full imagenet dataset [1] with 14,197,087 images in 21,841 classes. The model is trained by only random crop and mirror augmentation. The network is based on Inception-BN network [2], and added more capacity. This network runs roughly 2 times slower than standard Inception-BN Network. [1] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009. [2] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015).\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3e6ff39fcd75caaf20644bb1fde80811",
        "graph_path": "predict_net.pb",
        "weights_checksum": "a91e384bb76117832f3bd3058142080a",
        "weights_path": "init_net.pb"
      },
      "name": "InceptionBN-21K",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/pertusa/InceptionBN-21K-for-Caffe",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "10c2536572cf73045e1ef97a189f479c",
        "graph_path": "resnet101_predict_net.pb",
        "weights_checksum": "bda7af910d85e42f38d13aa76816eeeb",
        "weights_path": "resnet101_init_net.pb"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/leonardvandriel/caffe2_models",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "27591c0c6dddc01c0c235ab5783af0cb",
        "graph_path": "predict_net.pb",
        "weights_checksum": "220fad7b043d927d6baa259fd9e97d47",
        "weights_path": "init_net.pb"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1323dec34276a3016fdaa64c1aec4e07",
        "graph_path": "resnet152_predict_net.pb",
        "weights_checksum": "f0a184e29a712593a939b074b8c3c635",
        "weights_path": "resnet152_init_net.pb"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/leonardvandriel/caffe2_models",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fc4f24d1350f6bed93bd6387e8c8f3aa",
        "graph_path": "predict_net.pb",
        "weights_checksum": "064352d299c4565c8c71b4506388b377",
        "weights_path": "init_net.pb"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3be2756ee276ece8c6f381b41a2d8f0a",
        "graph_path": "predict_net.pb",
        "weights_checksum": "1fa62bf9bc55ec8e76b0e1f9d744b59d",
        "weights_path": "init_net.pb"
      },
      "name": "ResNet18",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification",
        "https://arxiv.org/abs/1512.03385",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1624badaa7ce7908123b5432aa3bc0bf",
        "graph_path": "predict_net.pb",
        "weights_checksum": "f06ddfb91db996636bd27ca1c6c143aa",
        "weights_path": "init_net.pb"
      },
      "name": "ResNet269",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385",
        "https://github.com/leonardvandriel/caffe2_models"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "57bc5f7493d089df7c4e35a673677a60",
        "graph_path": "resnet50_predict_net.pb",
        "weights_checksum": "080623733fd80e728c9c145d44de67ca",
        "weights_path": "resnet50_init_net.pb"
      },
      "name": "ResNet50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/leonardvandriel/caffe2_models",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c9c8593323a4fed955fd03bf4bdac242",
        "graph_path": "predict_net.pb",
        "weights_checksum": "4cfd2b7e599bfda6e0b5f8cb24cc639e",
        "weights_path": "init_net.pb"
      },
      "name": "ResNeXt101-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "8fc3f084dbb54fab7bd2eba14d5c1b5f",
        "graph_path": "predict_net.pb",
        "weights_checksum": "e0e4920723467a92271e5b6fb2eddfc7",
        "weights_path": "init_net.pb"
      },
      "name": "ResNeXt26-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4b002110c3fb188840708b8e1515b394",
        "graph_path": "predict_net.pb",
        "weights_checksum": "b057596968404b60cacad6d0ab2cbf8c",
        "weights_path": "init_net.pb"
      },
      "name": "ResNeXt50-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Squeezenet achieves similar results to AlexNet, at 50x fewer parameters and 1/500th the size. Small models are more feasible to deploy on hardware with limited memory, require less communication during distributed training, and are easier to distribute to clients. SqueezeNet achieves 57.5% and 80.3% top-1 and top-5 acuracty on ImageNet.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "de488013dfbf050461180c651dc5130b",
        "graph_path": "predict_net.pb",
        "weights_checksum": "2032c3f21fcadfe4ec5ab60cede24a46",
        "weights_path": "init_net.pb"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "SqueezeNet v1.1 has 2.4x less computation than v1.0, without sacrificing accuracy.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "694bfdd02e9ccb57bfc4acb451fbfb2d",
        "graph_path": "predict_net.pb",
        "weights_checksum": "a589d31d93c44d353ae2cd92af4d5a3f",
        "weights_path": "exec_net.pb"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/caffe2/models/tree/master/squeezenet",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.1"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 16-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "3b86a2354c75748c49a8e7b300dfdfea",
        "graph_path": "vgg16_predict_net.pb",
        "weights_checksum": "e62c909e16a449fe198a1f9fc26df8e1",
        "weights_path": "vgg16_init_net.pb"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/leonardvandriel/caffe2_models",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The MSO dataset"
      },
      "description": "The following model are finetuned on the Salient Object Subitizing dataset (~5000 images) with bounding box annotations. CNN models for the following CVPR'16 paper- Unconstrained Salient Object Detection via Proposal Subset Optimization J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price and R. Mech. CVPR, 2016.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "38a8c0e9b54834ff3980bb6063823f72",
        "graph_path": "predict_net.pb",
        "weights_checksum": "dd0e1a8721036933ea7c9626d028b97b",
        "weights_path": "init_net.pb"
      },
      "name": "VGG16_SOD",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/509111f8a00a9ece2c3d5dde6a750129",
        "https://github.com/jimmie33/SOD",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "http://cs-people.bu.edu/jmzhang/sod.html",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 19-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "a96a47109a0bc5eecaa27c9c75eb917c",
        "graph_path": "vgg19_predict_net.pb",
        "weights_checksum": "b83d401efdb1ae0c4cd6748301f8a2d3",
        "weights_path": "vgg19_init_net.pb"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/leonardvandriel/caffe2_models",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model was used for experiments with Wide Residual Networks (BMVC 2016) http://arxiv.org/abs/1605.07146 by Sergey Zagoruyko and Nikos Komodakis. Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts\n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3bc2e0c2792adc647d9e0c4b8feca610",
        "graph_path": "predict_net.pb",
        "weights_checksum": "680012c920ee212165ee071e195cd1e7",
        "weights_path": "init_net.pb"
      },
      "name": "WRN50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1605.07146",
        "https://github.com/szagoruyko/wide-residual-networks",
        "https://github.com/szagoruyko/functional-zoo/blob/master/imagenet-validation.py#L69-L88"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. \n",
      "framework": {
        "name": "Caffe2",
        "version": "0.8.1"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e64675df7d0c9758fc98b0e8ba3c3c25",
        "graph_path": "predict_net.pb",
        "weights_checksum": "43402ca1c1cedf51eed031013bd7346c",
        "weights_path": "init_net.pb"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/BVLC/caffe/pull/5665/files",
        "https://github.com/fchollet/deep-learning-models/blob/master/xception.py",
        "https://arxiv.org/abs/1610.02357"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0701ef8c0f83135b03a02ad09b23592a",
        "graph_path": "bvlc_alexnet_1.0/BVLC-Alexnet.dnn"
      },
      "name": "BVLC-AlexNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_alexnet",
        "https://github.com/BVLC/Caffe/wiki/Models-accuracy-on-ImageNet-2012-val",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model was trained by Sergio Guadarrama @sguada\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b63cd35b49a5042b5b433e804da0e035",
        "graph_path": "bvlc_googlenet_1.0/BVLC-GoogLeNet.dnn"
      },
      "name": "BVLC-GoogLeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_googlenet",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is the result of following the Tensorflow ImageNet model training instructions. It is a replication of the model described in the AlexNet publication with some differences. Differences: not training with the relighting data-augmentation; the order of pooling and normalization layers is switched (in TensorflowNet, pooling is done before normalization). This model is snapshot of iteration 310,000. The best validation performance during training was iteration 313,000 with validation accuracy 57.412% and loss 1.82328. This model obtains a top-1 accuracy 57.4% and a top-5 accuracy 80.4% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy still.) This model was trained by Jeff Donahue @jeffdonahue\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2db35bef87be3db0bcab4c141e127b9f",
        "graph_path": "bvlc_reference_caffenet_1.0/BVLC-Reference-CaffeNet.dnn"
      },
      "name": "BVLC-Reference-CaffeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_reference_Tensorflownet",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pure Tensorflow instantiation of the R-CNN model for ILSVRC13 detection. This model was made by transplanting the R-CNN SVM classifiers into a fc-rcnn classification layer, provided here as an off-the-shelf Tensorflow detector. Try the detection example to see it in action. N.B. For research purposes, make use of the official R-CNN package and not this example. This model was trained by Ross Girshick @rbgirshick\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "633f579519f6e95611429e6621e616e3",
        "graph_path": "bvlc_reference_rcnn_ilsvrc13_1.0/BVLC-Reference-RCNN_ILSVRC13.dnn"
      },
      "name": "BVLC-Reference-RCNN-ILSVRC13",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_reference_rcnn_ilsvrc13",
        "https://arxiv.org/abs/1311.2524",
        "http://nbviewer.jupyter.org/github/BVLC/Caffe/blob/master/examples/detection.ipynb"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c2f3c624fb06bdd1e6752012e2d7f19c",
        "graph_path": "inception_3.0/inception.dnn"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/cntk-model/tree/master/cls",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v4 which has a more uniform simplified architecture and more inception modules than Inception-v3.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "766fdb4aeb48440a8acf9ad5bcaee996",
        "graph_path": "inception_4.0/inception.dnn"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/kentsommer/keras-inceptionV4/issues/5"
      ],
      "version": "4.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3ed4412162e174c1acee2b852e5614a3"
      },
      "name": "Inception-ResNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/Tensorflow-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "6177f470b56e5bc164331023973a6301",
        "graph_path": "inceptionbn_21k_2.0/InceptionBN_21K.dnn"
      },
      "name": "InceptionBN-21K",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/pertusa/InceptionBN-21K-for-Tensorflow",
        "https://github.com/BVLC/Caffe/wiki/Model-Zoo",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 58.8% Top-1 Accuracy and 81.3% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "non-commercial",
      "model": {
        "graph_checksum": "67d89eb9d87045f30b6685c47ceb26b1",
        "graph_path": "network_in_network_1.0/NIN.dnn"
      },
      "name": "Network in Network",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/mavenlin/d802a5849de39225bcc6",
        "https://arxiv.org/abs/1312.4400"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c6fb3d695bb653dd0cb7cfc536bb7120",
        "graph_path": "resnet101_1.0/Resnet101.dnn"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e366d12078393fc89c282abbdb532864",
        "graph_path": "resnet101_2.0/Resnet101.dnn"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/Tensorflow-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "de5e053e35a9b55d06bdee66652f6da1",
        "graph_path": "resnet152_1.0/Resnet152.dnn"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e883b2ddd13d3d8d38156eb8e4ad06f8",
        "graph_path": "resnet152_2.0/Resnet152.dnn"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/Tensorflow-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "29f68817b3415016e079c1d1dcd94bee",
        "graph_path": "resnet18_1.0/resnet18.dnn"
      },
      "name": "ResNet18",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/Tensorflow-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "52c15b36e80a0ce6c71808a247f98229",
        "graph_path": "resnext50_32x4d_1.0/resnext50_32x4d.dnn"
      },
      "name": "ResNeXt50-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Squeezenet achieves similar results to AlexNet, at 50x fewer parameters and 1/500th the size. Small models are more feasible to deploy on hardware with limited memory, require less communication during distributed training, and are easier to distribute to clients. SqueezeNet achieves 57.5% and 80.3% top-1 and top-5 acuracty on ImageNet.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "28a1570c8774c6af76a9b458aee721f5",
        "graph_path": "squeezenet_1.0/squeezenet.dnn"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "SqueezeNet v1.1 has 2.4x less computation than v1.0, without sacrificing accuracy.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "4c80f333c273821d081e788f0a1761e1",
        "graph_path": "squeezenet_1.1/squeezenet.dnn"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360",
        "https://github.com/dividiti/ck-Tensorflow/tree/master/package/Tensorflowmodel-deepscale-squeezenet-1.1",
        "http://cknowledge.org/repo/web.php?template=cknowledge\u0026\u0026wcid=package:59414348b1bdedb5"
      ],
      "version": "1.1"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 16-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "bfac004627fa389d257e1f73d896fb25",
        "graph_path": "vgg16_1.0/vgg16.dnn"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/211839e770f7b538e2d8",
        "https://github.com/BVLC/Caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The MSO dataset"
      },
      "description": "The following model are finetuned on the Salient Object Subitizing dataset (~5000 images) with bounding box annotations. CNN models for the following CVPR'16 paper- Unconstrained Salient Object Detection via Proposal Subset Optimization J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price and R. Mech. CVPR, 2016.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "9773290b583835c6fe79019976ad4498",
        "graph_path": "vgg16_sod_1.0/vgg16_sod.dnn"
      },
      "name": "VGG16_SOD",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/509111f8a00a9ece2c3d5dde6a750129",
        "https://github.com/jimmie33/SOD",
        "https://github.com/BVLC/Caffe/wiki/Model-Zoo",
        "http://cs-people.bu.edu/jmzhang/sod.html",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The SOS Dataset"
      },
      "description": "VGG16 finetuned on the Salient Object Subitizing (SOS) dataset, which is described in the CVPR'15 paper: \"Salient Object Subitizing\"\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY-NC 4.0",
      "model": {
        "graph_checksum": "92a25a94a3c7c62a5e07a972585f6cff",
        "graph_path": "vgg16_sos_1.0/vgg16_sos.dnn"
      },
      "name": "VGG16_SOS",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/27c1c0a7736ba66c2395",
        "http://cs-people.bu.edu/jmzhang/sos.html",
        "https://github.com/BVLC/Caffe/wiki/Model-Zoo",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 19-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "d70fa65b0539e570425b40dff6ce1b7c",
        "graph_path": "vgg16_1.0/vgg16.dnn"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/3785162f95cd2d5fee77",
        "https://github.com/BVLC/Caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model was used for experiments with Wide Residual Networks (BMVC 2016) http://arxiv.org/abs/1605.07146 by Sergey Zagoruyko and Nikos Komodakis. Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts\n",
      "framework": {
        "name": "CNTK",
        "version": "2.3"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "19e372cda976629232902f938e350963",
        "graph_path": "wrn50_2.0/wrn50.dnn"
      },
      "name": "WRN50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/Tensorflow-model/tree/master/cls",
        "https://arxiv.org/abs/1605.07146",
        "https://github.com/szagoruyko/wide-residual-networks",
        "https://github.com/szagoruyko/functional-zoo/blob/master/imagenet-validation.py#L69-L88"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0c0f2da2728ec7e4e5d57e30c95351b1",
        "graph_path": "bvlc_alexnet-symbol.json",
        "weights_checksum": "0939a2a6f9e8749c0bc93663874d8bed",
        "weights_path": "bvlc_alexnet-0000.params"
      },
      "name": "BVLC-AlexNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
        "https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model was trained by Sergio Guadarrama @sguada\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e58431b8698ab4a09f7d6e70a8b4c1ce",
        "graph_path": "bvlc_googlenet-symbol.json",
        "weights_checksum": "c03c2b1409108b150c7cf5ce705436b0",
        "weights_path": "bvlc_googlenet-0000.params"
      },
      "name": "BVLC-GoogLeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 54.5% Top-1 Accuracy and 78.3% Top-5 accuracy on ILSVRC2012-Validation Set. This model is converted from caffenet provided in Caffe Model Zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "2510ba2fb49efa0cfe022a8e37bdc950",
        "graph_path": "bvlc_reference-caffenet_symbol.json",
        "weights_checksum": "b60a54f240480acad53a4dff9e7cbcaf",
        "weights_path": "bvlc_reference_caffenet-0000.params"
      },
      "name": "BVLC-Reference-CaffeNet",
      "output": {
        "description": "an output image net label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-caffenet.md",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pure Caffe instantiation of the R-CNN model for ILSVRC13 detection. This model was made by transplanting the R-CNN SVM classifiers into a fc-rcnn classification layer, provided here as an off-the-shelf Caffe detector. Try the detection example to see it in action. N.B. For research purposes, make use of the official R-CNN package and not this example. This model was trained by Ross Girshick @rbgirshick\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1b4f9a97a8e4771298d138fc9fea7288",
        "graph_path": "bvlc_reference_rcnn_ilsvrc13-symbol.json",
        "weights_checksum": "62582a8f18cd06565d8dc71703a82e5b",
        "weights_path": "bvlc_reference_rcnn_ilsvrc13-0000.params"
      },
      "name": "BVLC-Reference-RCNN-ILSVRC13",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_rcnn_ilsvrc13",
        "https://arxiv.org/abs/1311.2524",
        "http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b642582315c905d56b4e262065c3b8c1",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/dpn68/dpn68-symbol.json",
        "weights_checksum": "3784e62ee072ae8fc0fefab4c484d46a",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/dpn68/dpn68-0000.params"
      },
      "name": "DPN68",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fc13649b87e1cf86a367c4580ba59f78",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/dpn92/dpn92-symbol.json",
        "weights_checksum": "4b34d53d6b249fd8f9a17c798e1abaf4",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/dpn92/dpn92-0000.params"
      },
      "name": "DPN92",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5ebb2008e4525ee7f4edf0df85cee7af",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-v3/inception-v3-symbol.json",
        "weights_checksum": "6dca701662b169427b21050aaee0f761",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-v3/inception-v3-0000.params"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "More uniform  simplified  architecture  and  more  inception  modules than Inception-v3. Achieved 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "41a4a55cb14a01dafeddcb36adcc8f71",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-v4/inception-v4-symbol.json",
        "weights_checksum": "ec87c84b35ca958298a117cd2306be34",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-v4/inception-v4-0000.params"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/kentsommer/keras-inceptionV4/issues/5"
      ],
      "version": "4.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 72.5% Top-1 Accuracy and 90.8% Top-5 accuracy on ILSVRC2012-Validation Set. Single image prediction memory requirement: 10MB.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "93ea4544c19709161fec0051aea34885",
        "graph_path": "Inception-BN-symbol.json",
        "weights_checksum": "08bbf9b6c46e4a090ddaf039b76be630",
        "weights_path": "Inception-BN-0126.params"
      },
      "name": "Inception-BN",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-bn.md",
        "https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/symbols/inception-bn.py",
        "https://arxiv.org/abs/1512.00567",
        "https://arxiv.org/abs/1502.03167"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-ResNet-v2, a convolutional neural network (CNN) that achieves a new state of the art in terms of accuracy on the ILSVRC image classification benchmark. Inception-ResNet-v2 is a variation of our earlier Inception V3 model which borrows some ideas from Microsoft's ResNet papers. The full details of the model are in our arXiv preprint Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2cab0c9dac1a4c702819ca3fee4e4156",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-resnet-v2/inception-resnet-v2-symbol.json",
        "weights_checksum": "0268285f58c6d1863b383c412b576c07",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/inception-resnet-v2/inception-resnet-v2-0000.params"
      },
      "name": "Inception-ResNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet-21k"
      },
      "description": "This model is a pretrained model on full imagenet dataset with 14,197,087 images in 21,841 classes. The model is trained by only random crop and mirror augmentation. The network is based on Inception-BN network [2], and added more capacity. This network runs roughly 2 times slower than standard Inception-BN Network.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "8cb55dba6666df2d7c6e6baa43eb99c8",
        "graph_path": "Inception-symbol.json",
        "weights_checksum": "ff095008c3859c08ab328f774e0bc774",
        "weights_path": "Inception-0009.params"
      },
      "name": "InceptionBN-21K",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md",
        "https://arxiv.org/abs/1512.00567",
        "https://arxiv.org/abs/1502.03167"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "MultimediaCommons"
      },
      "description": "Geolocation model inspired by ideas presented in: PlaNet - Photo Geolocation with Convolutional Neural Networks (ECCV 2016), Tobias Weyand, Ilya Kostrikov, James Philbin https://research.google.com/pubs/pub45488.html\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "f1ea8e5e12c29b074626d85f109cc903",
        "graph_path": "RN101-5k500-symbol.json",
        "weights_checksum": "e206e84fd2b801a9faca47156638c009",
        "weights_path": "RN101-5k500-0012.params"
      },
      "name": "LocationNet",
      "output": {
        "description": "the output coordinates",
        "type": "coordinates"
      },
      "reference": [
        "https://github.com/multimedia-berkeley/tutorials",
        "https://arxiv.org/abs/1602.05314",
        "https://research.google.com/pubs/pub45488.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 58.8% Top-1 Accuracy and 81.3% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "46dcd37dafa691be464035d6a43d9fed",
        "graph_path": "nin-symbol.json",
        "weights_checksum": "6fd0c62875f9b2dd9745af60a38866d7",
        "weights_path": "nin-0000.params"
      },
      "name": "Network in Network",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/mavenlin/d802a5849de39225bcc6",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-nin.md",
        "https://arxiv.org/abs/1312.4400"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 101 layers. It is able to achieve 22.68% Top-1 Error and 6.58% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "752eea37001abd96429a458d5fe40584",
        "graph_path": "resnet-101-symbol.json",
        "weights_checksum": "2224d5eca1c9c022029c87770a79c70e",
        "weights_path": "resnet-101-0000.params"
      },
      "name": "o-ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 152 layers. It is able to achieve 22.25% Top-1 Error and 6.42% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "afc974eb8440cc7ce37fe49d051e805b",
        "graph_path": "resnet-152-symbol.json",
        "weights_checksum": "84c77aa3ae6dcd35aecf226788dc78f8",
        "weights_path": "resnet-152-0000.params"
      },
      "name": "o-ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012 dataset. This model is able to achieve 71.0% Top-1 Accuracy and 89.8% Top-5 accuracy on ILSVRC2012-Validation Set. The model is converted from caffemodel and they are widely used in transfer learning like object detection and image semantic segmentation.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "187dfba4acd3d304d896439778c3a516",
        "graph_path": "vgg16-symbol.json",
        "weights_checksum": "b932be3e94f94149ed391b3846d0f8f0",
        "weights_path": "vgg16-0000.params"
      },
      "name": "o-VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-vgg.md",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012 dataset. This model is able to achieve 71.0% Top-1 Accuracy and 89.8% Top-5 accuracy on ILSVRC2012-Validation Set. The model is converted from caffemodel and they are widely used in transfer learning like object detection and image semantic segmentation.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "2834785b11dfab63ce0a2b3a1992bd70",
        "graph_path": "vgg19-symbol.json",
        "weights_checksum": "4c42328f3ca4a3f0acf769ba65e7ea07",
        "weights_path": "vgg19-0000.params"
      },
      "name": "o-VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-vgg.md",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "62a5f507f926d80fe16abdb068c5a095",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet101/resnet-101-symbol.json",
        "weights_checksum": "eb217fadbefa91975f26155d3f30ab94",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet101/resnet-101-0000.params"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "136297a1618308322eed16c70548f967",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet101-v2/resnet101-v2-symbol.json",
        "weights_checksum": "34a705b0bb9c83fe2eafca68fdbcee2a",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet101-v2/resnet101-v2-0000.params"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "a1f16bbd4f77fa39ace4fb78a5f50da2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet152/resnet-152-symbol.json",
        "weights_checksum": "303fd5f4ddbd76d009db89afbfcb1813",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet152/resnet-152-0000.params"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 152 layers. It is able to achieve 22.25% Top-1 Error and 6.42% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "41135e4dc34fdb6c1ff1c5b5f2f0261e",
        "graph_path": "resnet152-v2-symbol.json",
        "weights_checksum": "8591df1d9f672aaa092b2d5bccc8aab8",
        "weights_path": "resnet152-v2-0000.params"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385",
        "https://github.com/KaimingHe/deep-residual-networks"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet-11k"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 152 layers and is trained on ImageNet 11K. Classes with less than 500 images are removed. The filtered dataset contains 11221 classes and 12.4 millions images. We randomly pick 50 images from each class as the validation set. Resnet152 achieves 58.4% Top-1 Error and 28.8% Top-5 error on the validation set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "b2105c7a31652d9bb7d0b4841ecb9154",
        "graph_path": "resnet-152-symbol.json",
        "weights_checksum": "a4cb744b366a1d4b1822cbcd3a30c852",
        "weights_path": "resnet-152-0000.params"
      },
      "name": "ResNet152-11k",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 18 layers. It is able to achieve 30.48% Top-1 Error and 10.92% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "8c0170478642cf0c1c77995769ecf329",
        "graph_path": "resnet-18-symbol.json",
        "weights_checksum": "dd1b81f8f818395036a3f9d17b31baea",
        "weights_path": "resnet-18-0000.params"
      },
      "name": "ResNet18",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 200 layers. It is able to achieve 22.14% Top-1 Error and 6.16% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "9ad2c70151b0022a4c9a8931f7f55093",
        "graph_path": "resnet-200-symbol.json",
        "weights_checksum": "f9027e4d7f170d66daa3d540c2b92709",
        "weights_path": "resnet-200-0000.params"
      },
      "name": "ResNet200",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "039f90aa515fe031606925381a99bd9a",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet269-v2/resnet269-v2-symbol.json",
        "weights_checksum": "a84738458fc34a7e356a2c91d8adcd55",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet269-v2/resnet269-v2-0000.params"
      },
      "name": "ResNet269",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 34 layers. It is able to achieve 27.20% Top-1 Error and 8.86% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "867b3d61d73991c6a7921d5dbc6e20db",
        "graph_path": "resnet-34-symbol.json",
        "weights_checksum": "04b652703e46ec6d123c3961b17bd5cc",
        "weights_path": "resnet-34-0000.params"
      },
      "name": "ResNet34",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5ebc93a69e2dd5f5b5d227c6de56c1bd",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet50/resnet-50-symbol.json",
        "weights_checksum": "85f120eea5a86f2420be054ef7315407",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnet50/resnet-50-0000.params"
      },
      "name": "ResNet50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 50 layers. It is able to achieve 24.39% Top-1 Error and 7.24% Top-5 Error on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "2989c88d1d6629b777949a3ae695a42e",
        "graph_path": "resnet-50-symbol.json",
        "weights_checksum": "246423771006aaf77acf68c99852f5b5",
        "weights_path": "resnet-50-0000.params"
      },
      "name": "ResNet50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/tornadomeet/ResNet",
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. This network consists of 101 layers. It is able to achieve 78.28% Top-1 Accuracy and 94.08% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "2dfc9be4851edd5fdeb0e57d3d5d1915",
        "graph_path": "resnext-101-symbol.json",
        "weights_checksum": "c4ccef9bc937298a91a3a032f992c5da",
        "weights_path": "resnext-101-0000.params"
      },
      "name": "ResNeXt101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1611.05431",
        "https://github.com/apache/incubator-mxnet/tree/master/example/image-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f3e5c3b8adb8cae1391694a873b7f4e2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext101-32x4d/resnext101-32x4d-symbol.json",
        "weights_checksum": "2146ce8953492e35d869160bbd9726b4",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext101-32x4d/resnext101-32x4d-0000.params"
      },
      "name": "ResNeXt101-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "702b4074257dd478df636c14de93124a",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext26-32x4d-priv/resnext26-32x4d-priv-symbol.json",
        "weights_checksum": "eaf38a10b575779197e977a721017948",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext26-32x4d-priv/resnext26-32x4d-priv-0000.params"
      },
      "name": "ResNeXt26-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. This network consists of 50 layers. It is able to achieve 76.89% Top-1 Accuracy and 93.32% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "685829baf29978f5b567afd7bf1407b6",
        "graph_path": "resnext-50-symbol.json",
        "weights_checksum": "e8b3a38c987e9623bd3a2ff088d1a0cd",
        "weights_path": "resnext-50-0000.params"
      },
      "name": "ResNeXt50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1611.05431",
        "https://github.com/apache/incubator-mxnet/tree/master/example/image-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e6c94dda4358519b6e3f03221715c466",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext50-32x4d/resnext50-32x4d-symbol.json",
        "weights_checksum": "20425f0c287093a58de3ace2c6359047",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/resnext50-32x4d/resnext50-32x4d-0000.params"
      },
      "name": "ResNeXt50-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012 dataset. It is able to achieve 55.4% Top-1 Accuracy and 78.8% Top-5 accuracy on ILSVRC2012-Validation Set. This model is famous for its small size(only 4.8M).\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "429b63a8781b85a85493d5f4162c6e79",
        "graph_path": "squeezenet_v1.0-symbol.json",
        "weights_checksum": "aa0145d7ef0976ba696085f82b9d67b0",
        "weights_path": "squeezenet_v1.0-0000.params"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-squeezenet.md",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "SqueezeNet v1.1 has 2.4x less computation than v1.0, without sacrificing accuracy.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "390f525c4da7286e817fdd7346eb5a7c",
        "graph_path": "squeezenet_v1.1-symbol.json",
        "weights_checksum": "05b1eb6acabdaaee37c9c9ff666c1b51",
        "weights_path": "squeezenet_v1.1-0000.params"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.1"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 16-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "3bf13add6eb9028e12a7247f0c38e2cd",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16/vgg16-symbol.json",
        "weights_checksum": "41a5e12a36f1b1fa84b736e27e8e4570",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16/vgg16-0000.params"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/211839e770f7b538e2d8",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The MSO dataset"
      },
      "description": "The following model are finetuned on the Salient Object Subitizing dataset (~5000 images) with bounding box annotations. CNN models for the following CVPR'16 paper- Unconstrained Salient Object Detection via Proposal Subset Optimization J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price and R. Mech. CVPR, 2016.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "f6448b57dc8af940b7530a065d7b5167",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16_sod/vgg16_sod-symbol.json",
        "weights_checksum": "d86c7bf027bc17aa050c0f2c3bb305ae",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16_sod/vgg16_sod-0000.params"
      },
      "name": "VGG16_SOD",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/509111f8a00a9ece2c3d5dde6a750129",
        "https://github.com/jimmie33/SOD",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "http://cs-people.bu.edu/jmzhang/sod.html",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The SOS Dataset"
      },
      "description": "VGG16 finetuned on the Salient Object Subitizing (SOS) dataset, which is described in the CVPR'15 paper: \"Salient Object Subitizing\"\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY-NC 4.0",
      "model": {
        "graph_checksum": "c37251f6d6f13c497f32f253c2fd6d88",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16_sos/vgg16_sos-symbol.json",
        "weights_checksum": "6a5212a37098787dcce46cde90136a7a",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg16_sos/vgg16_sos-0000.params"
      },
      "name": "VGG16_SOS",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/27c1c0a7736ba66c2395",
        "http://cs-people.bu.edu/jmzhang/sos.html",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 19-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "603f96ca639e60463c2b2a6980d0bc9f",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg19/vgg19-symbol.json",
        "weights_checksum": "1d63fb9727a7deaef2d2e6b30b07a9a9",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/vgg19/vgg19-0000.params"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/3785162f95cd2d5fee77",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model was used for experiments with Wide Residual Networks (BMVC 2016) http://arxiv.org/abs/1605.07146 by Sergey Zagoruyko and Nikos Komodakis. Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts\n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3004867b5176493e8c81fdb781d28af2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/wrn50-2/wrn50-2-symbol.json",
        "weights_checksum": "2e9c34d1c507b0eb75f46970f5ff6a89",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/wrn50-2/wrn50-2-0000.params"
      },
      "name": "WRN50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1605.07146",
        "https://github.com/szagoruyko/wide-residual-networks",
        "https://github.com/szagoruyko/functional-zoo/blob/master/imagenet-validation.py#L69-L88"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. \n",
      "framework": {
        "name": "MXNet",
        "version": "0.11.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "a639d872b1a7629d031c18b2a9185230",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/xception/xception-symbol.json",
        "weights_checksum": "31ac3beac6c5d6d3fac96448513007b8",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/mxnet/xception/xception-0000.params"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/BVLC/caffe/pull/5665/files",
        "https://github.com/fchollet/deep-learning-models/blob/master/xception.py",
        "https://arxiv.org/abs/1610.02357"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fdb525acaa8d3db0f87c5577901f53ee",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt",
        "weights_checksum": "29eb495b11613825c1900382f5286963",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel"
      },
      "name": "BVLC-AlexNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
        "https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model was trained by Sergio Guadarrama @sguada\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4a7281748d91b960ba86d681010d4aa2",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_googlenet/deploy.prototxt",
        "weights_checksum": "dc05671f0443708da1b0d0ae0e0c9cad",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel"
      },
      "name": "BVLC-GoogLeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is the result of following the Caffe ImageNet model training instructions. It is a replication of the model described in the AlexNet publication with some differences. Differences: not training with the relighting data-augmentation; the order of pooling and normalization layers is switched (in CaffeNet, pooling is done before normalization). This model is snapshot of iteration 310,000. The best validation performance during training was iteration 313,000 with validation accuracy 57.412% and loss 1.82328. This model obtains a top-1 accuracy 57.4% and a top-5 accuracy 80.4% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy still.) This model was trained by Jeff Donahue @jeffdonahue\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "955051d11e44bd29dd87a25dd766ec23",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_reference_caffenet/deploy.prototxt",
        "weights_checksum": "af678f0bd3cdd2437e35679d88665170",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel"
      },
      "name": "BVLC-Reference-CaffeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pure TensorRT instantiation of the R-CNN model for ILSVRC13 detection. This model was made by transplanting the R-CNN SVM classifiers into a fc-rcnn classification layer, provided here as an off-the-shelf TensorRT detector. Try the detection example to see it in action. N.B. For research purposes, make use of the official R-CNN package and not this example. This model was trained by Ross Girshick @rbgirshick\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "487294811977b91daa2a5e97651bb85c",
        "graph_path": "https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_reference_rcnn_ilsvrc13/deploy.prototxt",
        "weights_checksum": "42c1556d2d47a9128c4a90e0a9c5341c",
        "weights_path": "http://dl.caffe.berkeleyvision.org/bvlc_reference_rcnn_ilsvrc13.caffemodel"
      },
      "name": "BVLC-Reference-RCNN-ILSVRC13",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_rcnn_ilsvrc13",
        "https://arxiv.org/abs/1311.2524",
        "http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "db2d53bd6c310c1e2f83ae34341168a2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn68/deploy_dpn68-extra_no_ceil_mode.prototxt",
        "weights_checksum": "d8992cc89428578d65f930cdc6f52c81",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn68/dpn68-extra.caffemodel"
      },
      "name": "DPN68",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Dual Path Networks are highly efficient networks which combine the strength of both ResNeXt Aggregated Residual Transformations for Deep Neural Networks and DenseNets Densely Connected Convolutional Networks.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2c6116168692c8e37d6359141d39efa7",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn92/deploy_dpn92_no_ceil_mode.prototxt",
        "weights_checksum": "8ad72a531836f7f7c27e4f6d4c555b85",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/dpn92/dpn92.caffemodel"
      },
      "name": "DPN92",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/cypw/DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "two stream, 16 pixel prediction stride net, scoring 65.0 mIU on seg11valid trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models. These models are trained using extra data from Hariharan et al., but excluding SBD val. FCN-31s is fine-tuned from the ILSVRC-trained VGG-16 model, and the finer strides are then fine-tuned in turn. The \"at-once\" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "UNKNOWN",
      "model": {
        "graph_path": "https://raw.githubusercontent.com/shelhamer/fcn.berkeleyvision.org/master/voc-fcn16s/val.prototxt",
        "weights_path": "http://dl.caffe.berkeleyvision.org/fcn16s-heavy-pascal.caffemodel"
      },
      "name": "FCN-16s PASCAL",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1605.06211",
        "http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html",
        "https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn32s"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "single stream, 32 pixel prediction stride net, scoring 63.6 mIU on seg11valid. trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models. These models are trained using extra data from Hariharan et al., but excluding SBD val. FCN-32s is fine-tuned from the ILSVRC-trained VGG-16 model, and the finer strides are then fine-tuned in turn. The \"at-once\" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "UNKNOWN",
      "model": {
        "graph_path": "https://raw.githubusercontent.com/shelhamer/fcn.berkeleyvision.org/master/voc-fcn32s/val.prototxt",
        "weights_path": "http://dl.caffe.berkeleyvision.org/fcn32s-heavy-pascal.caffemodel"
      },
      "name": "FCN-32s PASCAL",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://arxiv.org/abs/1605.06211",
        "http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html",
        "https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/voc-fcn32s"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f6232b561b2ffb1ff2d7c4ff6957f66b",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v3/deploy_inception-v3.prototxt",
        "weights_checksum": "0516c5ad05b50f6c71a8891a0cb6c4e8",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v3/inception-v3.caffemodel"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "3.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v4 which has a more uniform  simplified  architecture  and  more  inception  modules than Inception-v3.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7d285a61595715cd7d0a503c7df1be02",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v4/deploy_inception-v4.prototxt",
        "weights_checksum": "c7bd2e74e0ab376671af6c388f622e80",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-v4/inception-v4.caffemodel"
      },
      "name": "Inception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py",
        "https://github.com/kentsommer/keras-inceptionV4/issues/5"
      ],
      "version": "4.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f4bfcf57b19470532cc3b244aec735e3",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-resnet-v2/deploy_inception-resnet-v2.prototxt",
        "weights_checksum": "6cda7beca27bb0c9e981ce21d5f27b18",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inception-resnet-v2/inception-resnet-v2.caffemodel"
      },
      "name": "Inception-ResNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://arxiv.org/abs/1602.07261",
        "https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "TODO\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "39865f78d0f94aad33c6bc8a779ff343",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inceptionbn-21k/deploy.prototxt",
        "weights_checksum": "6bf65186cee08b47ba6a767f5a5c8ce1",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/inceptionbn-21k/Inception21k.caffemodel"
      },
      "name": "InceptionBN-21K",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/pertusa/InceptionBN-21K-for-Caffe",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a pretrained model on ILSVRC2012[1] dataset. It is able to achieve 58.8% Top-1 Accuracy and 81.3% Top-5 accuracy on ILSVRC2012-Validation Set.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "non-commercial",
      "model": {
        "graph_checksum": "8c749056e1c3481e118d719a3344e590",
        "graph_path": "deploy.prototxt",
        "weights_checksum": "b568958c0dcf1d97cbcff4c22b02a2be",
        "weights_path": "nin_imagenet.caffemodel"
      },
      "name": "Network in Network",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/mavenlin/d802a5849de39225bcc6",
        "https://arxiv.org/abs/1312.4400"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3ec3d1a5d432ef54cb2d1afc9f83403a",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101/ResNet-101-deploy.prototxt",
        "weights_checksum": "3f8ccc93329ddc280b91efae09f71973",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101/ResNet-101-model.caffemodel"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "90e3a68da9f3eca03270487abc1ec4dd",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101-v2/deploy_resnet101-v2.prototxt",
        "weights_checksum": "780febbf5c47e40546d0d75a75cc8659",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet101-v2/resnet101-v2.caffemodel"
      },
      "name": "ResNet101",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3d2b961982298ab8400319cc83d34e73",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152/ResNet-152-deploy.prototxt",
        "weights_checksum": "654892a23df300ca47ebfe66b4cfaa1b",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152/ResNet-152-model.caffemodel"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e2eb074fe56c4854ad10938e37d76500",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152-v2/deploy_resnet152-v2.prototxt",
        "weights_checksum": "982360ab64c0a1a5ed7efafd84c55d0a",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet152-v2/resnet152-v2.caffemodel"
      },
      "name": "ResNet152",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b06f1f1dd4155c71108d819155956a98",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet18-priv/deploy_resnet18-priv_no_ceil_mode.prototxt",
        "weights_checksum": "0904d601fc930d4f0c62a2a95b3c3b93",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet18-priv/resnet18-priv.caffemodel"
      },
      "name": "ResNet18",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "05f07aed80e87d63db734f3f28a2b1f1",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet269-v2/deploy_resnet269-v2.prototxt",
        "weights_checksum": "d984f4774cf865728219ada8bfec0094",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet269-v2/resnet269-v2.caffemodel"
      },
      "name": "ResNet269",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/craftGBD/craftGBD",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "A Residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4ba3f945e7b86b07648e4f4351de0699",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet50/ResNet-50-deploy.prototxt",
        "weights_checksum": "44b20660c5948391734036963e855dd2",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnet50/ResNet-50-model.caffemodel"
      },
      "name": "ResNet50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/KaimingHe/deep-residual-networks",
        "https://arxiv.org/abs/1512.03385"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ec0c8e578040cbf76056deb41ba0af7d",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext101-32x4d/deploy_resnext101-32x4d_no_ceil_mode.prototxt",
        "weights_checksum": "5e5ec49b00d9f16d14ac6555dc992fcf",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext101-32x4d/resnext101-32x4d.caffemodel"
      },
      "name": "ResNeXt101-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "dde4af4f993b0db6c9b6dfec8fc3e615",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext26-32x4d-priv/deploy_resnext26-32x4d-priv_no_ceil_mode.prototxt",
        "weights_checksum": "a1d0f76137c4194aa4198df3a67fff26",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext26-32x4d-priv/resnext26-32x4d-priv.caffemodel"
      },
      "name": "ResNeXt26-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/soeaver/pytorch-classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "ResNeXt is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "bc71435f041d7f9eb5422e8d964b1dd3",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext50-32x4d/deploy_resnext50-32x4d_no_ceil_mode.prototxt",
        "weights_checksum": "d7ba773156d69ddb189aaf643356b9c5",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/resnext50-32x4d/resnext50-32x4d.caffemodel"
      },
      "name": "ResNeXt50-32x4d",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/caffe-model/tree/master/cls",
        "https://github.com/facebookresearch/ResNeXt",
        "https://github.com/facebookresearch/ResNeXt/blob/master/datasets/README.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "Squeezenet achieves similar results to AlexNet, at 50x fewer parameters and 1/500th the size. Small models are more feasible to deploy on hardware with limited memory, require less communication during distributed training, and are easier to distribute to clients. SqueezeNet achieves 57.5% and 80.3% top-1 and top-5 acuracty on ImageNet.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "ec33edbf17e082b3f5735fecc5a3d974",
        "graph_path": "deploy.prototxt",
        "weights_checksum": "bb9a2fd4be158e5b1e58a5cdc2b4aaa8",
        "weights_path": "squeezenet_v1.0.caffemodel"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "SqueezeNet v1.1 has 2.4x less computation than v1.0, without sacrificing accuracy.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "BAIR",
      "model": {
        "graph_checksum": "1e67cb171e16a4d66fd331409d2f7b0d",
        "graph_path": "https://raw.githubusercontent.com/rai-project/carml-models/master/caffe/squeezenet_v1.1/deploy.prototxt",
        "weights_checksum": "0357e4e11d173c72a01615888826bc8e",
        "weights_path": "https://github.com/DeepScale/SqueezeNet/raw/master/SqueezeNet_v1.1/squeezenet_v1.1.caffemodel"
      },
      "name": "SqueezeNet",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/DeepScale/SqueezeNet",
        "https://arxiv.org/abs/1602.07360",
        "https://github.com/dividiti/ck-tensorrt/tree/master/package/caffemodel-deepscale-squeezenet-1.1",
        "http://cknowledge.org/repo/web.php?template=cknowledge\u0026\u0026wcid=package:59414348b1bdedb5"
      ],
      "version": "1.1"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 16-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "c70550f8203a4eaae53d7c39ef34c92d",
        "graph_path": "https://gist.githubusercontent.com/ksimonyan/211839e770f7b538e2d8/raw/ded9363bd93ec0c770134f4e387d8aaaaa2407ce/VGG_ILSVRC_16_layers_deploy.prototxt",
        "weights_checksum": "441315b0ff6932dbfde97731be7ca852",
        "weights_path": "http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/211839e770f7b538e2d8",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The MSO dataset"
      },
      "description": "The following model are finetuned on the Salient Object Subitizing dataset (~5000 images) with bounding box annotations. CNN models for the following CVPR'16 paper- Unconstrained Salient Object Detection via Proposal Subset Optimization J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price and R. Mech. CVPR, 2016.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "MIT",
      "model": {
        "graph_checksum": "eac6d2978cff65319c7ef34aa89d760c",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sod/deploy.prototxt",
        "weights_checksum": "9a7dce6fc980c29d962f7c2758b2761d",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sod/VGG16_SOD_finetune.caffemodel"
      },
      "name": "VGG16_SOD",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/509111f8a00a9ece2c3d5dde6a750129",
        "https://github.com/jimmie33/SOD",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "http://cs-people.bu.edu/jmzhang/sod.html",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "The SOS Dataset"
      },
      "description": "VGG16 finetuned on the Salient Object Subitizing (SOS) dataset, which is described in the CVPR'15 paper: \"Salient Object Subitizing\"\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "hidden": true,
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY-NC 4.0",
      "model": {
        "graph_checksum": "eb820e9e4d6f35a961666acf0f2f15a2",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sos/deploy.prototxt",
        "weights_checksum": "fa7886a2fcf21b0c49fe2f2b20fe5643",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/vgg16_sos/VGG16_SalObjSub.caffemodel"
      },
      "name": "VGG16_SOS",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/jimmie33/27c1c0a7736ba66c2395",
        "http://cs-people.bu.edu/jmzhang/sos.html",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "hidden": "true",
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The model is an improved version of the 19-layer model used by the VGG team in the ILSVRC-2014 competition.\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "CC BY 4.0 (commercial use is allowed)",
      "model": {
        "graph_checksum": "ccbbdda59210208be39f8974f5b5765e",
        "graph_path": "https://gist.githubusercontent.com/ksimonyan/3785162f95cd2d5fee77/raw/f43eeefc869d646b449aa6ce66f87bf987a1c9b5/VGG_ILSVRC_19_layers_deploy.prototxt",
        "weights_checksum": "b5c644beabd7cf06bdd9065cfd674c97",
        "weights_path": "http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://gist.github.com/ksimonyan/3785162f95cd2d5fee77",
        "https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014",
        "https://arxiv.org/abs/1409.1556"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model was used for experiments with Wide Residual Networks (BMVC 2016) http://arxiv.org/abs/1605.07146 by Sergey Zagoruyko and Nikos Komodakis. Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts\n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "efd258e3c28cab1f0e0a36c77aab2dec",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/wrn50-2/deploy_wrn50-2_no_ceil_mode.prototxt",
        "weights_checksum": "ea6c90d2f16a140d038359626bf78211",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/wrn50-2/wrn50-2.caffemodel"
      },
      "name": "WRN50",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/tensorrt-model/tree/master/cls",
        "https://arxiv.org/abs/1605.07146",
        "https://github.com/szagoruyko/wide-residual-networks",
        "https://github.com/szagoruyko/functional-zoo/blob/master/imagenet-validation.py#L69-L88"
      ],
      "version": "2.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "An interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. \n",
      "framework": {
        "name": "TensorRT",
        "version": "2.1.2"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d0df3aaaa43e4b82e4794b89420b7067",
        "graph_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/xception/deploy_xception.prototxt",
        "weights_checksum": "cad1c76e17becaeb0bde0351cfb6dba8",
        "weights_path": "http://s3.amazonaws.com/store.carml.org/models/caffe/xception/xception.caffemodel"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "feature"
      },
      "reference": [
        "https://github.com/soeaver/tensorrt-model/tree/master/cls",
        "https://github.com/BVLC/caffe/pull/5665/files",
        "https://github.com/fchollet/deep-learning-models/blob/master/xception.py",
        "https://arxiv.org/abs/1610.02357"
      ],
      "version": "1.0"
    }
  ]
}